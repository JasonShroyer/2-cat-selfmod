# Composite Network Tutorial

Welcome to the **Composite Network Tutorial** for our 2‑Categorical Self‑Modification Framework!  
In this notebook, you'll learn how to build and test composite neural networks using our library.  
We model neural network layers as *1‑morphisms* (with explicit parameter spaces) and allow internal modifications via *2‑morphisms* (reparameterizations).  

---

## Table of Contents

1. [Introduction & Theoretical Background](#Introduction)
2. [Building a Simple Composite Network](#SimpleCompositeNetwork)
3. [Running the Network and Examining Gradients](#RunningNetwork)
4. [A Brief Look at Reparameterization](#Reparameterization)
5. [Next Steps and Further Exploration](#NextSteps)
6. [Conclusion](#Conclusion)


## Introduction & Theoretical Background <a name="Introduction"></a>

In our framework:

- **1‑Morphisms** represent neural network layers along with their parameters.  
  For example, a layer might be defined as a function \(f: (P, x) \to y\), where \(P\) is the parameter space (e.g., weights and biases) and \(x\) is the input.

- **2‑Morphisms** capture *reparameterizations*: transformations of the internal representation (the parameter space) that leave the overall function \(f\) unchanged.  
  This allows a network to "rewrite its own code" internally without affecting its observable behavior.

- **Differentiable Morphisms** integrate with PyTorch (or your favorite deep learning library) so that standard gradient-based optimization applies.

In this tutorial, we will focus on building a composite network that chains two differentiable layers. Then, we will test the forward pass and backpropagation to ensure everything is working as expected.

---

### Key Concepts Recap

- **ParametricMorphism**: Encapsulates a layer with its parameter space and a function (e.g., scaling, linear transformation).
- **DifferentiableMorphism**: A PyTorch-friendly version that registers parameters and supports autograd.
- **Reparam2Morphism**: Represents a reparameterization (not the focus of this tutorial, but an important concept for future work).

Let's dive in!

---


# Cell 1: Import necessary modules and our library components

# Import standard libraries
import numpy as np
import torch
import torch.nn as nn

# Import our library components.
# (Ensure that your package is installed in editable mode)
from basecat.objects import CatObject, TupleParamSpace
from basecat.diff_morphisms import DifferentiableMorphism


## Building a Simple Composite Network <a name="SimpleCompositeNetwork"></a>

In this section, we will build a simple composite network that consists of two layers:

1. **Layer 1:** A differentiable morphism that scales its input.
2. **Layer 2:** Another differentiable morphism that scales the output further.

The overall function is defined as:
\[
y = \text{Layer2}(\text{Layer1}(x))
\]
For example, if Layer1 multiplies by 2 and Layer2 multiplies by 3, then for an input \(x = 1.0\) the output will be \(6.0\).

Let's write the code!


## Running the Network and Examining Gradients <a name="RunningNetwork"></a>

Now that we have defined our composite network, let’s perform a forward pass, compute a loss, and run backpropagation.  
This will help us verify that gradients are being computed correctly through our differentiable morphisms.

In the following cell, we'll:
- Compute a dummy loss (e.g., sum of outputs).
- Perform backpropagation.
- Print the gradients for each layer.


# Cell 3: Forward and Backward Pass

# Forward pass
y = net(x)
print(f"Output before backward pass: {y.item()}")

# Compute a simple loss (sum of outputs)
loss = y.sum()
print("Loss:", loss.item())

# Perform backward pass
loss.backward()

# Print gradients of each layer's parameter
print("Layer1 gradient:", net.layer1.param.grad)
print("Layer2 gradient:", net.layer2.param.grad)


## A Brief Look at Reparameterization <a name="Reparameterization"></a>

One of the unique aspects of our framework is the ability to modify a network’s internal parameterization (i.e., reparameterize) without changing its output function.  
Although this notebook focuses on building composite networks, here’s a quick demonstration of a simple identity reparameterization:

- We have a source layer and a target layer, both implementing the same function.
- A 2‑morphism (reparameterization) is applied using the identity function so that:
  \[
  f'(p', x) = f(r(p'), x)
  \]
  where \(r\) is the identity function.

Below is a brief example.


# Cell 4: Demonstration of Identity Reparameterization

from basecat.morphisms import ParametricMorphism
from basecat.reparam2morph import Reparam2Morphism

# Define dummy identity function for reparameterization.
def dummy_layer_identity(theta, x):
    return x

# Create domain and codomain objects.
X = CatObject("InputSpace", shape=(1,))
Y = CatObject("OutputSpace", shape=(1,))
param_space = TupleParamSpace((1.0,))  # simple scalar parameter

# Create two identical morphisms.
morph_source = ParametricMorphism(X, Y, param_space, dummy_layer_identity, name="Source")
morph_target = ParametricMorphism(X, Y, param_space, dummy_layer_identity, name="Target")

# Identity reparameterization: r(p') = p'
identity_reparam = lambda p: p

# Create a 2-morphism.
rho = Reparam2Morphism(morph_source, morph_target, identity_reparam, name="IdentityReparam")

# Check the reparameterization condition.
if rho.check_commute(test_samples=5, tol=1e-6, use_torch=False):
    print("Identity reparameterization check passed!")
else:
    print("Identity reparameterization check failed!")


## Next Steps and Further Exploration <a name="NextSteps"></a>

This tutorial has shown you how to:

- Build a composite network from differentiable layers.
- Run a forward and backward pass to verify that gradients are computed.
- Briefly explore the idea of reparameterization using a simple identity mapping.

**Next steps you can try:**

1. **Experiment with Nonlinear Activations:**  
   Incorporate additional layers (such as a ReLU activation) to see how they affect network behavior.

2. **Build More Advanced Networks:**  
   Extend your composite network by adding branches or skip connections.  
   Check how reparameterizations can be applied in more complex scenarios.

3. **Develop a Mini Training Loop:**  
   Create a small dataset, define a loss function, and train your composite network. Observe how the parameters update over time.

4. **Explore Reparameterization Strategies:**  
   Try designing and testing non-trivial reparameterizations (e.g., weight tying, neuron permutation) and see how they impact internal representations.

5. **Enhance Documentation:**  
   Expand this notebook or create additional ones to cover more advanced topics, such as the theoretical background and practical use cases of 2‑morphisms.

---

## Conclusion <a name="Conclusion"></a>

In this tutorial, we walked through:
- The theoretical basis of our 2‑categorical framework.
- Building a simple composite network using differentiable morphisms.
- Running forward and backward passes and examining gradients.
- A brief demonstration of reparameterization.

This forms the foundation for further experimentation and development in our self‑modifying AI framework. Happy coding, and enjoy exploring the possibilities!

---

*End of Notebook*


